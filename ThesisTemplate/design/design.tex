\chapter{Design}\label{ch:Design}

\section{Introduction}
\subsection{Problem Definition and outline}
In many types of datasets, a class imbalance is frequent as there are much fewer incidences of the target outcome label than of the "normal" ones . Class imbalance in data  can lead algorithms to misidentified instances, giving a "normal" label to a target instance, for example, because the data on which the algorithm was trained did not contain enough instances of the "target" label.  There are many real-life instances where the wrong classification of an instance could have devastating consequences and the aim of this project is to:
\begin{enumerate}
\item compare the performance of several different algorithms on imbalanced datasets
\item identify potential solutions that will allow the algorithms to "learn more efficiently" even when a class is underrepresented
\item test the different solutions and compare the performance of the algorithms on the same datasets once the solutions have been implemented
\end{enumerate}
\subsection{Class Imbalance in healthcare datasets}
Healthcare-related datasets are particularly sensitive to the class imbalance problem and this can be more or less severe depending on the condition that is being studied. Some diseases have a more frequent occurrence than others, for example certain types of cancer have a very low incidence, especially when looking at the general population, while others are much more common, though their incidence in the population at large will still create a class imbalance. There are however situations where the class imbalance is reversed and incidence of "sickness" tends to be higher than that of "health", and this has been reported in EHR data; since EHR data are formed from doctors' patient records, there tends to be an over-representation of unhealthy cases versus healthy ones in those dataset. In either case, the intrinsic bias of the data towards one of the classes will create issues when training an algorithms as there are insufficient cases of one class to learn from. This will likely result in the algorithm failing to identify properly cases of "sick" label for example and mislabelling them as "healthy". This problem could be further amplified if the class distribution in the data used for training is very different from the class distribution in the testing or live data, on which the algorithm will be used.
In cases of medical diagnostic this could have dramatic consequences as the wrong diagnosis could be issued to a patient. 

\subsection{Proposed solutions}

what are the known solutions to the problem
\subsection{Experimental design outline}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ThesisTemplate/usingLatex/images/Chapter3Figures001.jpeg}
    \caption{Schematic of the experimental design of the study.}
    \label{fig:my_label}
\end{figure}

\section{Dataset Choice and Preliminary Exploration}
\subsection{Datasets}
\subsubsection{How were the dataset chosen?}
The Kaggle repository of datasets (https://www.kaggle.com/datasets) was searched for datasets with the tags "healthcare", "health", "oncology and cancer" and "health sciences".
This search returned 223 datasets. Genomic/proteomic datasets were discarded as they are best suited to clustering tasks. Datasets where classification tasks could be carried out were kept and further examined.
A total of 9 datasets were retained for use in the analysis.

\subsubsection{Brief Overview of the datasets}
The datasets chosen for the analaysis were:
\begin{itemize}
    \item Breast Cancer Wisconsin Data Set
    \item Indian Liver Patient Records
    \item MRI and Alzheimers
    \item Pima Indian dataset
    \item Cervical Cancer Risk Classification
    \item Lower back pain symptom dataset
    \item Heart Attack Prediction
    \item Autism Screening
    \item Fertility dataset
\end{itemize}

All 9 datasets allow classification tasks to be carried out. 

\subsection{Preliminary Exploration of the chosen datasets}
\subsubsection{Defining features}
some quick visualisation of data 
\subsubsection{Class distribution}
show class distribution/imbalance for the chosen datasets
\begin{table}
\begin{tabular}{ |p{5cm}|p{1.5cm}| p{1.5cm} | p{2 cm}|}
 \hline
 \multicolumn{4}{|c|}{Datasets} \\
 \hline
 Dataset Name & Instances & Columns & Class Distribution \\
 \hline
 Breast Cancer Wisconsin Data Set\footnote{https://www.kaggle.com/uciml/breast-cancer-wisconsin-data}  & 569 & 32 & 
357-212\\
Indian Liver Patient Records\footnote{https://www.kaggle.com/uciml/indian-liver-patient-records} &  583 & 11  & 416-167 \\
MRI and Alzheimers\footnote{https://www.kaggle.com/jboysen/mri-and-alzheimers\#oasis\_longitudinal.csv} & 416 & 12 & 316-100\\
Pima Indian dataset\footnote{https://www.kaggle.com/uciml/pima-indians-diabetes-database} & 768 & 9  & ?\\
Cervical Cancer Risk Classification\footnote{https://www.kaggle.com/loveall/cervical-cancer-risk-classification  } & 858 &  36 & \\
Lower back pain symptom dataset\footnote{ https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset} & 310 & 13  & \\
Heart Attack Prediction\footnote{https://www.kaggle.com/imnikhilanand/heart-attack-prediction} & 294 & 14  & 188-106\\
Autism Screening\footnote{https://www.kaggle.com/faizunnabi/autism-screening/home} &704 & 21 & \\
Fertility dataset\footnote{https://www.kaggle.com/gabbygab/fertility-data-set} &100  & 10 & \\
 \hline
\end{tabular}
\caption{Summary of datasets chosen for the analysis}
\label{table:1}
\end{table}
% here need to sort out the footnote%

\section{Choice of algorithms used in evaluation}
\subsection{Quick overview of algorithms}
This works focuses on supervised learning classification tasks. Several supervised machine learning algorithms have shown promises in computer-aided diagnostic (CAD) \citep{Dua:2014dz}:
\begin{itemize}
    \item decision trees (DT) such as Random Forest are easy to comprehend and relatively easy to implement but are challenging to apply to non linear problems \citep{Gray:2013eh}
    \item support vector machine \citep{Naraei:ct}
    \item k-NN \citep{Liu:2011dz}
    \item Bayesian models \citep{Dangare:ut}
\end{itemize}

\subsection{Reasons for choosing those algorithms}
The algorithms listed above have demonstrated high performance in classifying tasks on healthcare data, even in the case of imbalanced datasets \citep{Dua:2014dz}. They also utilise different mathematical approaches to classification and therefore are expected to behave differently from one another.



\section{Proposed avenues of exploration}
\subsection{Overview of typical methods to address class imbalance}
The class imbalance problem was discussed in chapter 2 along with some of the techniques used to compensate for its effects. These techniques are discussed here further and can be separated in data-level solutions and algorithm-level solutions:
\begin{enumerate}
    \item data-level solutions %insert ref here from Sun et al.
    \begin{itemize}
        \item undersampling the prevalent class 
        \item oversampling the minority class
        \item detecting the sub-concepts constituting a class and  oversampling each concept to balance the overall distribution
    \end{itemize}
    \item algorithm level solutions
    the solutions here are highly dependent upon the algorithm and in order o develop an algorithmic solution, knowledge of the classifier learning algorithm and the application domain is necessary as well as a thorough comprehension on why the learning algorithm fails when the class distribution of available data is uneven
    \begin{itemize}
        \item choosing an appropriate inductive bias (e.g. for decision trees, adjusting the pruning technique)
        \item for SVMs, different penalty constants can be used for different classes
    \end{itemize}
    \item cost-sensitive learning
    Cost-sensitive classification considers the varying costs of different misclassification types i.e. various penalties are applied for classifying one class as another. In this situation, the learning aims to minimize high cost misclassifications and the total cost of misclassification.
    Cost-sensitivity can be obtained through:
    \begin{itemize}
        \item weighting the data space
        \item making a classifier cost-sensitive by adjusting parameters to minimize the cost of misclassification
        \item using Bayes Risk Theory to assign each sample to its lowest risk class
    \end{itemize}
\end{enumerate}
\subsection{Testing the Impact of the use of these methods}
\subsubsection{First test the algorithms on the dataset without any modifications}
Details of procedure which will be employed to get a baseline of performance for each dataset and algorithms before applying methods which will help correct the class imbalance

\subsubsection{Applying the methods to each datasets and algorithms}
Details of procedure which will be employed to adjust/correct class imbalance on the datasets


\section{Evaluating Performance of algorithms and correction methods}

\subsection{Metrics for evaluation} % here use discussion of what is best appropriate to evaluate performance in context of class imbalance 
Quick overview of each metric plus how helpful they are 
\subsubsection{Accuracy}
\subsubsection{Calibration}
\subsubsection{Discrimination}
\subsubsection{Negative Predictive Value}
\subsubsection{Precision}
\subsubsection{Recall}
\subsubsection{Specificity}

\subsection{Choice of most relevant metrics}
\subsubsection{which metrics will be most helpful in the healthcare context}
\subsubsection{create a combined metric for this evaluation}
this is maybe an idea, where a composite metric from the most appropriate metrics could be used?

\subsection{Comparing performance of algorithms for each dataset before and after applying the methods to compensate for class imbalance}
In this section detail how the performance of algorithms can be compared before any adjustments for class imbalances are made and after; details how each algorithms will be compared etc

\section{Technical Requirements}
\subsection{Language}
\subsection{Software}

\section{Discussion of results}
\subsection{Results obtained}
This section to detail how the results will be aggregated and discussed; how do we know we have achieved what we set out to do
\subsection{Driving factors for the results}
Discuss how different factors will be assessed
\subsection{Conclusions: most effective way to deal with class imbalance}
Details of how the most effective solutions will be decided from the results




\section{Conclusions}

This chapter has outlined the experimental design chosen for this project. 
What problem?
How do we evaluate?



