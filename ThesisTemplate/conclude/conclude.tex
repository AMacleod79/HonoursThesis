\chapter{Conclusion}\label{ch:Conclusion}
This project has investigated potential data-level solutions that can be implemented to reduce the negative impact of imbalanced datasets on the performance of algorithms in machine learning, particularly in the context of healthcare and medical diagnostics.\newline
The approach was to first determine the baseline performance of Random Forest on 8 datasets (plus modified versions of two of the chosen sets with greater class imbalance). Second, under-sampling of the data was carried out in various proportions and the performance metrics measured again. In a third experiment, over-sampling by SMOTE was carried out and the performance metrics measured. Accuracy, Precision, Sensitivity and F1 score were recorded in all cases and compared between experiments.
The outcomes of each experiment were detailed in chapter 5 and the work carried out in this project shows that no single solution improved the performance of Random Forest for all datasets.\newline
Performance must first be clearly defined and with medical diagnostic in mind, it was decided that sensitivity would be considered a more important parameter than precision. Overall accuracy was still considered important though small decreases in accuracy could be tolerated if sensitivity was significantly increased. The F1 score is a more ambiguous measure to consider as it is the harmonic mean of both precision and sensitivity. An increase in the F1 score is beneficial though it is unclear if it is due to a large increase in sensitivity and a constant or slightly reduced precision, or the opposite, or both measure increasing. It does give an idea of the balance between both measures and a decrease in F1 score largely points to a decrease in algorithm performance.\newline

\section{Conclusions}
The results to the experiments carried out showed that for a given dataset under-sampling was a better data-level solution to apply to improve the performance of Random Forest, as has been suggested by others \citep{Rekha:2019uu}, though the opposite has also been found. Working with the same Cervical Cancer dataset and a Decision Tree algorithm, Al-Wesabi and colleagues \citep{AlWesabi:hx} found that oversampling was far superior to under-sampling in improving algorithm performance. The parameters for SMOTE used here specifically for the Cervical Cancer dataset lead to a large reduction of data even though the data then contain the same number of positive and negative cases. This may explain the discrepancy between the results of this projects and those reported by Al-Wesabi and colleagues.\newline
The extent to which under-sampling improved the performance of an Random Forest was not constant, although it almost consistently improved sensitivity. In contrast, the effect of SMOTE was more constant across the datasets but less pronounced.\newline
The results obtained with the Fertility dataset and the modified Heart Attack and modified Low Back Pain suggest that another type of algorithm, such a SVM or k-NN may have performed better. The baseline experiment failed to correctly identify any positive cases for the Fertility dataset and this may have been because Random Forest was not best suited to this data environment. This theory could be examined further (see future work) and inform the current results.\newline


\section{Future Work}
Should more time be spent on this project, it would be interesting to investigate the effects of different parameters for SMOTE on the performance of the algorithm.
An attractive idea is that of devising a hybrid strategy for example under-sampling the majority class after having applied SMOTE to the dataset as reported by Liu and colleagues \citep{Liu:2006ij}. This could further improve the performance of the algorithm compared to what is obtain with either technique.\newline
The original plan for this project, as detailed in chapter 3, outlined the comparison of the performance of several algorithms using the same datasets and both under and over-sampling. This has not been carried out due to lack of time, however this could be informative to assess whether the data-level solutions can improve the performance of several different algorithms when used in the same way, i.e. would SMOTE applied with the same parameters to a given dataset result in performance improvement for both a Decision Tree algorithm and k-NN or SVM.\newline
Another aspect discussed in Chapter 3, but not carried out in this project are algorithm level solutions that can be experimented with in order to improve the performance of an algorithm for a given dataset. This type of solution would most likely be very data-specific, but very suited to an algorithm such as SVM where learning cost can be factored in as parameter.

